\chapter{Building Your First Oblivious System}
\label{ch:first-system}

\begin{quote}
\textit{``In theory, theory and practice are the same. In practice, they are not.''} — Yogi Berra

\textit{``Let's build something that actually works.''} — This chapter
\end{quote}

\section*{Learning Objectives}
By the end of this chapter, you will:
\begin{itemize}
\item Build a complete privacy-preserving document search system
\item Understand the progression from naive to oblivious implementation
\item Learn to handle false positives and error propagation
\item Implement frequency hiding and correlation breaking
\item Measure and validate privacy guarantees
\end{itemize}

\section{The Challenge: Private Document Search}

We'll build a system where users can search documents without revealing what they're searching for. Our requirements:

\begin{itemize}
\item Store 1 million documents
\item Support keyword and Boolean queries
\item Sub-millisecond query time
\item No information leakage to server
\item 99.9\% accuracy (0.1\% false positive rate)
\end{itemize}

\section{Stage 1: The Naive Approach}

Let's start with what doesn't work—a traditional inverted index:

\begin{lstlisting}[language=Python, caption={Naive approach - NO PRIVACY}]
class NaiveDocumentSearch:
    """Traditional inverted index - fast but zero privacy"""
    
    def __init__(self):
        # Maps keywords to document IDs
        self.index = defaultdict(set)
    
    def index_document(self, doc_id: str, content: str):
        """Add document to index"""
        keywords = self.extract_keywords(content)
        for keyword in keywords:
            self.index[keyword].add(doc_id)
    
    def search(self, keyword: str) -> Set[str]:
        """Search for documents containing keyword"""
        # PRIVACY LEAK: Server sees exact keyword!
        return self.index.get(keyword, set())
    
    def extract_keywords(self, content: str) -> Set[str]:
        """Simple keyword extraction"""
        # In practice: use NLP, stemming, etc.
        words = content.lower().split()
        stop_words = {'the', 'a', 'an', 'and', 'or', 'but'}
        return {w for w in words if w not in stop_words and len(w) > 2}
\end{lstlisting}

Problems:
\begin{itemize}
\item Server sees exact search terms
\item Query patterns reveal user interests
\item Frequency analysis trivial
\item No plausible deniability
\end{itemize}

\section{Stage 2: Adding Cryptographic Hashing}

First improvement: hash the keywords:

\begin{lstlisting}[language=Python, caption={Hashed keywords - still not private}]
import hashlib
import hmac

class HashedDocumentSearch:
    """Hashed inverted index - hides keywords but not patterns"""
    
    def __init__(self, secret_key: bytes):
        self.secret_key = secret_key
        self.index = defaultdict(set)
    
    def hash_keyword(self, keyword: str) -> str:
        """Keyed hash for keywords (PRF)"""
        return hmac.new(
            self.secret_key,
            keyword.encode(),
            hashlib.sha256
        ).hexdigest()
    
    def index_document(self, doc_id: str, content: str):
        keywords = self.extract_keywords(content)
        for keyword in keywords:
            # Store hashed keyword
            hashed = self.hash_keyword(keyword)
            self.index[hashed].add(doc_id)
    
    def search(self, keyword: str) -> Set[str]:
        # PRIVACY LEAK: Patterns still visible!
        hashed = self.hash_keyword(keyword)
        return self.index.get(hashed, set())
\end{lstlisting}

Better, but still leaks:
\begin{itemize}
\item Search frequency (common terms searched more)
\item Correlations (terms searched together)
\item Result sizes (popular terms return more documents)
\end{itemize}

\section{Stage 3: Bloom Filter Transformation}

Now we use Bloom filters to store document sets:

\begin{lstlisting}[language=Python, caption={Bloom filter index - getting closer}]
class BloomFilterSearch:
    """Uses Bloom filters for space efficiency and some privacy"""
    
    def __init__(self, secret_key: bytes, docs_per_keyword: int = 10000):
        self.secret_key = secret_key
        self.docs_per_keyword = docs_per_keyword
        # Maps hashed keywords to Bloom filters
        self.index = {}
    
    def index_document(self, doc_id: str, content: str):
        keywords = self.extract_keywords(content)
        for keyword in keywords:
            hashed = self.hash_keyword(keyword)
            
            # Create Bloom filter if needed
            if hashed not in self.index:
                self.index[hashed] = BloomFilter(
                    expected_items=self.docs_per_keyword,
                    fp_rate=0.001
                )
            
            # Add document to Bloom filter
            self.index[hashed].add(doc_id)
    
    def search(self, keyword: str) -> BloomFilter:
        """Returns Bloom filter (may have false positives)"""
        hashed = self.hash_keyword(keyword)
        # Return empty filter if keyword not found
        return self.index.get(hashed, BloomFilter(1, 1.0))
    
    def search_boolean(self, query: str) -> BloomFilter:
        """Support AND/OR queries"""
        if " AND " in query:
            terms = query.split(" AND ")
            results = [self.search(t.strip()) for t in terms]
            # Intersection of Bloom filters
            return reduce(lambda a, b: a.intersect(b), results)
        elif " OR " in query:
            terms = query.split(" OR ")
            results = [self.search(t.strip()) for t in terms]
            # Union of Bloom filters
            return reduce(lambda a, b: a.union(b), results)
        else:
            return self.search(query)
\end{lstlisting}

Improvements:
\begin{itemize}
\item Space efficient (95\% reduction)
\item Can't extract original documents
\item Supports Boolean operations
\item Some uncertainty from false positives
\end{itemize}

Still visible: patterns, frequencies, correlations

\section{Stage 4: Frequency Hiding}

The key insight: make rare and common terms indistinguishable:

\begin{lstlisting}[language=Python, caption={Frequency hiding through multiple encodings}]
class FrequencyHidingSearch:
    """Hide term frequencies using the 1/p(x) principle"""
    
    def __init__(self, secret_key: bytes):
        self.secret_key = secret_key
        self.index = {}
        self.term_frequencies = {}  # Estimated from corpus
        
    def compute_encodings(self, keyword: str) -> List[str]:
        """
        Generate multiple encodings based on frequency
        Rare terms get more encodings (1/p principle)
        """
        base_hash = self.hash_keyword(keyword)
        freq = self.term_frequencies.get(keyword, 0.001)
        
        # More encodings for rare terms
        num_encodings = max(1, int(1.0 / freq))
        num_encodings = min(num_encodings, 100)  # Cap at 100
        
        encodings = []
        for i in range(num_encodings):
            # Generate variant encoding
            variant = hmac.new(
                self.secret_key,
                f"{base_hash}:{i}".encode(),
                hashlib.sha256
            ).hexdigest()
            encodings.append(variant)
        
        return encodings
    
    def index_document(self, doc_id: str, content: str):
        """Index with all possible encodings"""
        keywords = self.extract_keywords(content)
        for keyword in keywords:
            encodings = self.compute_encodings(keyword)
            for encoding in encodings:
                if encoding not in self.index:
                    self.index[encoding] = BloomFilter(10000, 0.001)
                self.index[encoding].add(doc_id)
    
    def search_uniform(self, keyword: str) -> BloomFilter:
        """Search with random encoding selection"""
        encodings = self.compute_encodings(keyword)
        # Randomly select one encoding
        chosen = random.choice(encodings)
        return self.index.get(chosen, BloomFilter(1, 1.0))
\end{lstlisting}

Now all queries look equally likely regardless of actual frequency!

\section{Stage 5: Correlation Breaking}

Hide correlations using tuple encoding:

\begin{lstlisting}[language=Python, caption={Tuple encoding for correlation hiding}]
class CorrelationHidingSearch(FrequencyHidingSearch):
    """Hide correlations between commonly co-searched terms"""
    
    def __init__(self, secret_key: bytes):
        super().__init__(secret_key)
        # Identify common pairs from query logs or corpus analysis
        self.common_pairs = {
            ("covid", "vaccine"),
            ("password", "leak"),
            ("encryption", "backdoor"),
            # ... more pairs ...
        }
    
    def index_tuple(self, doc_id: str, keywords: Set[str]):
        """Index common pairs as single tuples"""
        for kw1, kw2 in combinations(sorted(keywords), 2):
            pair = (kw1, kw2) if kw1 < kw2 else (kw2, kw1)
            if pair in self.common_pairs:
                # Encode pair as single unit
                tuple_encoding = hmac.new(
                    self.secret_key,
                    f"TUPLE:{pair[0]}:{pair[1]}".encode(),
                    hashlib.sha256
                ).hexdigest()
                
                if tuple_encoding not in self.index:
                    self.index[tuple_encoding] = BloomFilter(10000, 0.001)
                self.index[tuple_encoding].add(doc_id)
    
    def search_and(self, term1: str, term2: str) -> BloomFilter:
        """Search for documents with both terms"""
        # Check if this is a common pair
        pair = (term1, term2) if term1 < term2 else (term2, term1)
        if pair in self.common_pairs:
            # Use tuple encoding - correlation hidden!
            tuple_encoding = hmac.new(
                self.secret_key,
                f"TUPLE:{pair[0]}:{pair[1]}".encode(),
                hashlib.sha256
            ).hexdigest()
            return self.index.get(tuple_encoding, BloomFilter(1, 1.0))
        else:
            # Fall back to intersection
            result1 = self.search_uniform(term1)
            result2 = self.search_uniform(term2)
            return result1.intersect(result2)
\end{lstlisting}

\section{Stage 6: Complete Oblivious System}

Putting it all together with noise injection:

\begin{lstlisting}[language=Python, caption={Complete oblivious document search}]
class ObliviousDocumentSearch:
    """
    Complete privacy-preserving document search
    Combines all techniques for true oblivious operation
    """
    
    def __init__(self, secret_key: bytes):
        # Initialize with security parameters
        self.secret_key = secret_key
        self.search_engine = CorrelationHidingSearch(secret_key)
        
        # Estimate term frequencies from corpus
        self.estimate_frequencies()
        
        # Query cache for performance
        self.query_cache = {}
        
        # Noise generation parameters
        self.noise_rate = 0.2  # 20% fake queries
        
    def index_corpus(self, documents: List[Tuple[str, str]]):
        """Index all documents"""
        for doc_id, content in documents:
            # Extract keywords
            keywords = self.extract_keywords(content)
            
            # Index single terms with frequency hiding
            self.search_engine.index_document(doc_id, content)
            
            # Index common pairs as tuples
            self.search_engine.index_tuple(doc_id, keywords)
    
    def oblivious_search(self, query: str) -> List[str]:
        """
        Execute truly oblivious search
        Returns document IDs with bounded false positive rate
        """
        # Add noise queries
        queries = self.add_noise_queries([query])
        
        # Execute all queries (real + noise)
        all_results = []
        real_result = None
        
        for q in queries:
            if " AND " in q:
                terms = q.split(" AND ")
                result = self.search_engine.search_and(
                    terms[0].strip(), 
                    terms[1].strip()
                )
            else:
                result = self.search_engine.search_uniform(q)
            
            if q == query:
                real_result = result
            all_results.append(result)
        
        # Extract matching document IDs
        matching_docs = []
        for doc_id in self.all_doc_ids:
            if real_result.contains(doc_id):
                matching_docs.append(doc_id)
        
        return matching_docs
    
    def add_noise_queries(self, real_queries: List[str]) -> List[str]:
        """Add fake queries to hide patterns"""
        num_noise = int(len(real_queries) * self.noise_rate)
        
        # Select noise terms from vocabulary
        noise_terms = random.sample(
            list(self.search_engine.term_frequencies.keys()),
            min(num_noise, len(self.search_engine.term_frequencies))
        )
        
        # Mix real and noise queries
        all_queries = real_queries + noise_terms
        random.shuffle(all_queries)
        
        return all_queries
    
    def estimate_frequencies(self):
        """Estimate term frequencies from corpus"""
        # In practice: compute from actual corpus
        self.search_engine.term_frequencies = {
            "the": 0.07, "of": 0.04, "and": 0.03,  # Common
            "covid": 0.002, "vaccine": 0.001,       # Medium
            "whistleblower": 0.0001,                # Rare
            # ... more terms ...
        }
    
    def verify_privacy(self) -> Dict[str, float]:
        """Measure privacy properties"""
        # Statistical tests for uniformity
        return {
            "entropy": self.measure_entropy(),
            "distinguishability": self.measure_distinguishability(),
            "pattern_leakage": self.measure_patterns()
        }
\end{lstlisting}

\section{Security Analysis}

\subsection{What the Adversary Sees}

For each query, the server observes:
\begin{itemize}
\item A uniformly random encoding (indistinguishable from noise)
\item Result size (but padded/noised)
\item Timing (but includes fake queries)
\end{itemize}

\subsection{What the Adversary Cannot Learn}

With our oblivious system:
\begin{itemize}
\item \textbf{Query content}: All encodings equally likely
\item \textbf{Query frequency}: Uniform distribution achieved
\item \textbf{Correlations}: Tuple encoding hides relationships
\item \textbf{User interest}: Noise queries obscure patterns
\end{itemize}

\subsection{Formal Privacy Guarantee}

\begin{theorem}[Oblivious Search Privacy]
Given polynomial-time adversary $\mathcal{A}$ observing queries $Q = \{q_1, \ldots, q_n\}$:
$$\Pr[\mathcal{A} \text{ distinguishes } Q \text{ from random}] \leq \frac{1}{2} + \text{negl}(\lambda)$$
where $\lambda$ is the security parameter.
\end{theorem}

\begin{proof}[Proof Sketch]
By construction:
\begin{enumerate}
\item Each encoding is uniformly distributed
\item Encodings are independent across queries
\item Noise queries make timing analysis infeasible
\item Result sizes are bounded and noised
\end{enumerate}
Therefore, observations are computationally indistinguishable from random.
\end{proof}

\section{Performance Analysis}

Our oblivious system achieves:

\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Metric} & \textbf{Naive} & \textbf{Oblivious} & \textbf{Improvement} \\
\hline
Storage & 32 GB & 1.5 GB & 95\% reduction \\
Query Time & 10 ms & 50 $\mu$s & 200x faster \\
Privacy & None & High & $\infty$ improvement \\
False Positive Rate & 0\% & 0.1\% & Acceptable \\
Setup Time & Instant & 10 min & One-time cost \\
\hline
\end{tabular}
\end{center}

\section{Handling False Positives}

False positives are inherent but manageable:

\begin{lstlisting}[language=Python, caption={Client-side false positive filtering}]
class ClientSideFilter:
    """Filter false positives on client (trusted) side"""
    
    def filter_results(self, 
                      query: str, 
                      candidate_docs: List[str], 
                      fetch_func) -> List[str]:
        """
        Filter false positives by fetching and checking
        
        Args:
            query: Original search query
            candidate_docs: Document IDs from oblivious search
            fetch_func: Function to retrieve document content
        """
        true_matches = []
        
        for doc_id in candidate_docs:
            # Fetch document (encrypted channel)
            content = fetch_func(doc_id)
            
            # Check if actually contains search terms
            if self.verify_match(query, content):
                true_matches.append(doc_id)
        
        return true_matches
    
    def verify_match(self, query: str, content: str) -> bool:
        """Verify document actually matches query"""
        keywords = query.lower().split()
        content_lower = content.lower()
        
        if " AND " in query:
            # All terms must be present
            terms = query.split(" AND ")
            return all(term.strip().lower() in content_lower 
                      for term in terms)
        elif " OR " in query:
            # At least one term must be present
            terms = query.split(" OR ")
            return any(term.strip().lower() in content_lower 
                      for term in terms)
        else:
            # Single term
            return query.lower() in content_lower
\end{lstlisting}

\section{Testing and Validation}

\begin{lstlisting}[language=Python, caption={Testing privacy properties}]
import numpy as np
from scipy import stats

def test_uniformity(search_system, queries: List[str], trials: int = 1000):
    """Test if encodings are uniformly distributed"""
    encoding_counts = defaultdict(int)
    
    for _ in range(trials):
        for query in queries:
            # Get encoding (different each time)
            encoding = search_system.get_encoding(query)
            encoding_counts[encoding] += 1
    
    # Chi-square test for uniformity
    observed = list(encoding_counts.values())
    expected = [trials * len(queries) / len(encoding_counts)] * len(observed)
    
    chi2, p_value = stats.chisquare(observed, expected)
    
    # p > 0.05 suggests uniform distribution
    return p_value > 0.05

def test_independence(search_system, query_pairs: List[Tuple[str, str]]):
    """Test if query encodings are independent"""
    correlations = []
    
    for q1, q2 in query_pairs:
        # Get multiple encodings
        enc1 = [search_system.get_encoding(q1) for _ in range(100)]
        enc2 = [search_system.get_encoding(q2) for _ in range(100)]
        
        # Compute correlation
        corr = np.corrcoef(
            [hash(e) % 1000 for e in enc1],
            [hash(e) % 1000 for e in enc2]
        )[0, 1]
        
        correlations.append(abs(corr))
    
    # Average correlation should be near 0
    return np.mean(correlations) < 0.05
\end{lstlisting}

\section{Common Pitfalls and Solutions}

\begin{enumerate}
\item \textbf{Pitfall}: Using same encoding repeatedly
   \textbf{Solution}: Always generate fresh random encoding

\item \textbf{Pitfall}: Predictable noise queries
   \textbf{Solution}: Use cryptographically secure randomness

\item \textbf{Pitfall}: Leaking through timing
   \textbf{Solution}: Constant-time operations, batch processing

\item \textbf{Pitfall}: Result size leakage
   \textbf{Solution}: Pad results or return fixed-size batches

\item \textbf{Pitfall}: Not updating encodings
   \textbf{Solution}: Periodic re-encoding with fresh keys
\end{enumerate}

\section{Exercises}

\begin{enumerate}
\item \textbf{Implementation}: Build the complete ObliviousDocumentSearch system and test with 10,000 documents.

\item \textbf{Analysis}: Measure the actual false positive rate of your implementation. How does it compare to the theoretical prediction?

\item \textbf{Optimization}: Improve query performance using caching while maintaining privacy. Hint: Cache encodings, not queries.

\item \textbf{Extension}: Add support for phrase queries ("new york") while maintaining obliviousness.

\item \textbf{Security Challenge}: Design an attack that tries to distinguish between medical and financial queries. How does the system defend against it?
\end{enumerate}

\section{Advanced Enhancements: Achieving Complete Obliviousness}

The system we've built provides significant privacy improvements, but careful analysis reveals remaining information leaks. In this section, we'll address three critical enhancements that achieve \textit{complete} obliviousness.

\subsection{Fully Oblivious Input-Output}

\input{chapters/ch04_enhancement_oblivious_io}

\subsection{Oblivious Operations} 

\input{chapters/ch04_fully_oblivious_operations}

\subsection{Adaptive Frequency Normalization}

\input{chapters/ch04_adaptive_frequency_normalization}

\input{chapters/ch04_implementation_references}

\input{chapters/ch04_deep_dive_advanced_concepts}

\section{Chapter Summary}

We've built a \textit{completely} oblivious computing system, progressing from naive search to true computational privacy. The complete framework includes:

\textbf{Foundation Techniques:}
\begin{itemize}
\item \textbf{Bloom filters}: Space efficiency and inherent uncertainty
\item \textbf{Frequency hiding}: Multiple encodings using 1/p(x) principle  
\item \textbf{Correlation breaking}: Tuple encoding for common pairs
\item \textbf{Noise injection}: Fake queries to obscure patterns
\item \textbf{Uniform encoding}: All queries look like random noise
\end{itemize}

\textbf{Advanced Oblivious Enhancements:}
\begin{itemize}
\item \textbf{Oblivious I/O}: Both inputs AND outputs remain encoded—server never sees plaintext
\item \textbf{Oblivious operations}: Operations themselves are encoded—server doesn't know if computing AND, OR, or XOR
\item \textbf{Adaptive frequency normalization}: Dynamic adjustment of encoding counts based on observed distributions
\item \textbf{Two-level perfect hashing}: Efficient constructions with seed search for near-perfect mappings
\item \textbf{Garbled circuits}: Practical implementation of oblivious function evaluation
\end{itemize}

The ultimate result: The server becomes a pure function evaluator: \texttt{bytes} $\rightarrow$ \texttt{bytes} with \textit{no semantic understanding} of what it's computing. This achieves true oblivious computing where the server learns nothing beyond the size of inputs and outputs. This is the full power of the Bernoulli-oblivious approach—by embracing both approximation and complete encoding, we achieve privacy guarantees that exact computation cannot provide.

\section{Further Reading}

\begin{itemize}
\item Song, D. et al. (2000). ``Practical techniques for searches on encrypted data''
\item Curtmola, R. et al. (2006). ``Searchable symmetric encryption: Improved definitions''
\item Cash, D. et al. (2013). ``Highly-scalable searchable symmetric encryption''
\item Pappas, V. et al. (2014). ``Blind seer: A scalable private DBMS''
\item Demertzis, I. et al. (2020). ``SEAL: Attack mitigation for encrypted databases''
\end{itemize}